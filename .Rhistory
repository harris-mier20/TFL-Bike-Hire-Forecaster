dim(tmin.vect) <- c(12, length(tmin.vect)/12)
tmax.vect <- WeatherData$tmax
dim(tmax.vect) <- c(12, length(tmax.vect)/12)
# Calculate standard deviation and quantiles for tmin.vect
# apply function to apply an function to each row of the vector
# all info stored in tmin_summary
tmin_summary <- apply(tmin.vect, 1, function(x) {
# The c() function in the code is used to concatenate or
#combine the results of the standard deviation and quantiles
#and mean into a single vector
c(
# calculate standard deviation of each row
sd(x),
# calculate the quartiles of each row
#again, c() used to combine the quartiles into a single vector
quantile(x, c(0.05, 0.95)),
#calculate the mean of each month
mean(x)
)
})
#repeat for tmax
tmax_summary <- apply(tmax.vect, 1, function(x) {
c(
sd(x),
quantile(x, c(0.05, 0.95)),
mean(x)
)
})
# Extract the means from tmin_summary
means <- tmin_summary[4, ]
#extact the quarltiles and build the polygon coordinates
polygon <- tmin_summary[2, ]
polygon2 <- tmin_summary[3, ]
polygon <- c(polygon, rev(polygon2))
months <- c(1:12, 12:1)
plot(means, type="l", col="blue", xaxt = "n", xlab = "", ylab = "",
ylim = c(-3, 22))
# Set x-axis labels
axis(1, at = 1:12, labels = months)
#load weather data
WeatherData <- read.csv("http://pierrepinson.com/wp-content/uploads/2023/10/Heathrow-weather-data-1948-2022.csv", header=TRUE)
# define the axis labels for months
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#Extract tmin and tmax and reshape them to become matrices
#where each line is for a month, and each column is for a year
# and find mean of each month
tmin.vect <- WeatherData$tmin
dim(tmin.vect) <- c(12, length(tmin.vect)/12)
tmax.vect <- WeatherData$tmax
dim(tmax.vect) <- c(12, length(tmax.vect)/12)
# Calculate standard deviation and quantiles for tmin.vect
# apply function to apply an function to each row of the vector
# all info stored in tmin_summary
tmin_summary <- apply(tmin.vect, 1, function(x) {
# The c() function in the code is used to concatenate or
#combine the results of the standard deviation and quantiles
#and mean into a single vector
c(
# calculate standard deviation of each row
sd(x),
# calculate the quartiles of each row
#again, c() used to combine the quartiles into a single vector
quantile(x, c(0.05, 0.95)),
#calculate the mean of each month
mean(x)
)
})
#repeat for tmax
tmax_summary <- apply(tmax.vect, 1, function(x) {
c(
sd(x),
quantile(x, c(0.05, 0.95)),
mean(x)
)
})
# Extract the means from tmin_summary
means <- tmin_summary[4, ]
#extact the quarltiles and build the polygon coordinates
polygon <- tmin_summary[2, ]
polygon2 <- tmin_summary[3, ]
polygon <- c(polygon, rev(polygon2))
months_numerical <- c(1:12, 12:1)
plot(means, type="l", col="blue", xaxt = "n", xlab = "", ylab = "",
ylim = c(-3, 22))
# Set x-axis labels
axis(1, at = 1:12, labels = months)
polygon(months_numerial, polygon, col = "blue")
#load weather data
WeatherData <- read.csv("http://pierrepinson.com/wp-content/uploads/2023/10/Heathrow-weather-data-1948-2022.csv", header=TRUE)
# define the axis labels for months
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#Extract tmin and tmax and reshape them to become matrices
#where each line is for a month, and each column is for a year
# and find mean of each month
tmin.vect <- WeatherData$tmin
dim(tmin.vect) <- c(12, length(tmin.vect)/12)
tmax.vect <- WeatherData$tmax
dim(tmax.vect) <- c(12, length(tmax.vect)/12)
# Calculate standard deviation and quantiles for tmin.vect
# apply function to apply an function to each row of the vector
# all info stored in tmin_summary
tmin_summary <- apply(tmin.vect, 1, function(x) {
# The c() function in the code is used to concatenate or
#combine the results of the standard deviation and quantiles
#and mean into a single vector
c(
# calculate standard deviation of each row
sd(x),
# calculate the quartiles of each row
#again, c() used to combine the quartiles into a single vector
quantile(x, c(0.05, 0.95)),
#calculate the mean of each month
mean(x)
)
})
#repeat for tmax
tmax_summary <- apply(tmax.vect, 1, function(x) {
c(
sd(x),
quantile(x, c(0.05, 0.95)),
mean(x)
)
})
# Extract the means from tmin_summary
means <- tmin_summary[4, ]
#extact the quarltiles and build the polygon coordinates
polygon <- tmin_summary[2, ]
polygon2 <- tmin_summary[3, ]
y <- c(polygon, rev(polygon2))
x <- c(1:12, 12:1)
#plot the means
plot(means, type="l", col="blue", xaxt = "n", xlab = "", ylab = "",
ylim = c(-3, 22))
polygon(x, y, col = "blue")
# Set x-axis labels
axis(1, at = 1:12, labels = months)
#load weather data
WeatherData <- read.csv("http://pierrepinson.com/wp-content/uploads/2023/10/Heathrow-weather-data-1948-2022.csv", header=TRUE)
# define the axis labels for months
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#Extract tmin and tmax and reshape them to become matrices
#where each line is for a month, and each column is for a year
# and find mean of each month
tmin.vect <- WeatherData$tmin
dim(tmin.vect) <- c(12, length(tmin.vect)/12)
tmax.vect <- WeatherData$tmax
dim(tmax.vect) <- c(12, length(tmax.vect)/12)
# Calculate standard deviation and quantiles for tmin.vect
# apply function to apply an function to each row of the vector
# all info stored in tmin_summary
tmin_summary <- apply(tmin.vect, 1, function(x) {
# The c() function in the code is used to concatenate or
#combine the results of the standard deviation and quantiles
#and mean into a single vector
c(
# calculate standard deviation of each row
sd(x),
# calculate the quartiles of each row
#again, c() used to combine the quartiles into a single vector
quantile(x, c(0.05, 0.95)),
#calculate the mean of each month
mean(x)
)
})
#repeat for tmax
tmax_summary <- apply(tmax.vect, 1, function(x) {
c(
sd(x),
quantile(x, c(0.05, 0.95)),
mean(x)
)
})
# Extract the means from tmin_summary
means <- tmin_summary[4, ]
#extact the quarltiles and build the polygon coordinates
polygon <- tmin_summary[2, ]
polygon2 <- tmin_summary[3, ]
y <- c(polygon, rev(polygon2))
x <- c(1:12, 12:1)
#plot the means
plot(means, type="l", col="blue", xaxt = "n", xlab = "", ylab = "",
ylim = c(-3, 22))
polygon(x, y, col=rgb(1,0,0,0.2))
# Set x-axis labels
axis(1, at = 1:12, labels = months)
runApp('Desktop/DE4/Data to Product/Tutorial 2/mastering-shiny-exercises.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/mastering-shiny-exercises.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/mastering-shiny-exercises.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/mastering-shiny-exercises.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/mastering-shiny-exercises.R')
library(shiny); runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-data-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-data-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/app-weather-statistics-v1.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
runApp('Desktop/DE4/Data to Product/Tutorial 2/weather-dashboard.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
View(ec1_data)
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
every_500th
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
View(ec1_data)
length(ec1_data)
length(ec1_data$Date)
ec1_data$Date[1321]
ec1_data$Date[1321*3/4]
ec1_data$Date[1321*2/4]
ec1_data$Date[1321*2/3]
ec1_data$Date[1321*1/3]
ec1_data$Date[1]
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
library(shiny); runApp('Desktop/TFL-Bike-Hire-Forecaster/app-master.R')
dates
date
# Read the data from the CSV files and extract the station names
data <- read.csv("data/daily-activity-by-postcode.csv")
setwd("/Users/Harris/Desktop/TFL-Bike-Hire-Forecaster")
# Read the data from the CSV files and extract the station names
data <- read.csv("data/daily-activity-by-postcode.csv")
sim_parameters <- read.csv("data/capacity-simulation/simulation-parameters.csv")
sim_results <- read.csv("data/capacity-simulation/simulation-results.csv")
#load in the file that defines all the postcode region borders
source("short-forecasting.R")
#collect data on all stations including a calculation of the activity per station
#for each postcode
postcode_labels <- c("ec1","ec2","ec3","ec4","wc1","wc2")
activity_means <-colMeans(data[3:8])
activity_sd <- sapply(data[3:8], sd)
activity_max <- numeric()
#find the max value in each column
for (i in 3:8){
activity_max = c(activity_max, max(data[[i]]))
}
#hard code the number of stations in each postcode and calculate the
#ratio of activity to number of stations in each postcode
n_stations <- c(29,23,9,14,29,23)
activity_aps <- unlist(Map("/", activity_max, n_stations))
activity_sd <- round(activity_sd, digits = 0)
#round the data
activity_means <- round(activity_means, digits = 0)
activity_aps <- round(activity_aps, digits = 1)
#create data frame with statistics on each station
postcode_statistics <- data.frame("Postcode" = postcode_labels,
"Stations" = n_stations,
"Mean" = activity_means,
"sd" = activity_sd,
"max" = activity_max,
"Ratio"= activity_aps)
#create empty list to fill with emojis and colours to describe data
emojis <- list()
colours <- list()
#add emoji and colour information to describe the data - for rendering on the UI
for (i in 1: length(postcode_statistics$Ratio)){
if (postcode_statistics$Ratio[i] >= 225){
emojis[i] <- "angry"
colours[i] <- "red"
} else if (postcode_statistics$Ratio[i] >= 200){
emojis[i] <- "disappointed"
colours[i] <- "orange"
} else {
emojis[i] <- "smile"
colours[i] <- "green"
}
}
#append the new info to the data frame
postcode_statistics$Emoji <- emojis
postcode_statistics$Colour <- colours
#define function to smooth the data with exponential smoothing
smooth_data <- function(data,alpha,starting_value){
#define parameters
aa <- alpha
l0 <- starting_value
#initiate vector to store smoothed data
smoothed_data <- numeric(length(data))
#loop through the data and perform exponential smoothing
for (i in 1:length(smoothed_data)) {
smoothed_data[i] <- aa * data[i] + (1 - aa) * l0
l0 <- smoothed_data[i]
}
return(smoothed_data)
}
#create a data frame with the data for WC1
#one column for the data, one column for the smoothed data
dates <- data$Date
raw_wc1 <- data$WC1
smooth_wc1 <- smooth_data(raw_wc1, 0.1, 2000)
wc1_data <- data.frame("Date" = dates,
"Raw" = raw_wc1,
"Smooth" = smooth_wc1)
#create a data frame with the data for WC2
#one column for the data, one column for the smoothed data
dates <- data$Date
raw_wc2 <- data$WC2
smooth_wc2 <- smooth_data(raw_wc2, 0.1, 2000)
wc2_data <- data.frame("Date" = dates,
"Raw" = raw_wc2,
"Smooth" = smooth_wc2)
#create a data frame with the data for EC1
#one column for the data, one column for the smoothed data
dates <- data$Date
raw_ec1 <- data$EC1
smooth_ec1 <- smooth_data(raw_ec1, 0.1, 2000)
ec1_data <- data.frame("Date" = dates,
"Raw" = raw_ec1,
"Smooth" = smooth_ec1)
#create a data frame with the data for EC2
#one column for the data, one column for the smoothed data
dates <- data$Date
raw_ec2 <- data$EC2
smooth_ec2 <- smooth_data(raw_ec2, 0.1, 2000)
ec2_data <- data.frame("Date" = dates,
"Raw" = raw_ec2,
"Smooth" = smooth_ec2)
#create a data frame with the data for EC3
#one column for the data, one column for the smoothed data
dates <- data$Date
raw_ec3 <- data$EC3
smooth_ec3 <- smooth_data(raw_ec3, 0.1, 2000)
ec3_data <- data.frame("Date" = dates,
"Raw" = raw_ec3,
"Smooth" = smooth_ec3)
#create a data frame with the data for EC4
#one column for the data, one column for the smoothed data
dates <- data$Date
raw_ec4 <- data$EC4
smooth_ec4 <- smooth_data(raw_ec4, 0.1, 2000)
ec4_data <- data.frame("Date" = dates,
"Raw" = raw_ec4,
"Smooth" = smooth_ec4)
### - handle the long term forecasting of the app - ###
#create function that interpolates data to restore the full length of data points from a string
interpolate_data <- function(original_data, new_length) {
original_length <- length(original_data)
# Create a sequence of indices for the original data
original_indices <- seq(1, original_length, length.out = original_length)
# Create a sequence of indices for the new data
new_indices <- seq(1, original_length, length.out = new_length)
# Use the approx function for linear interpolation
interpolated_data <- approx(original_indices, original_data, xout = new_indices)$y
return(interpolated_data)
}
#function that takes data and completes Holt Winter forecasting
holtwinter <- function(data){
#define model parameters
m  <- 28
aa <- 0.5
bb <- 0.1
gg <- 0.9
ti <- 50
lv <- 2000
bv <- 0
sv <- rep(0,m+1)
fc <- 60
cut <- 50
#reduce the data down for the model
reduced <- data[seq(1, length(data), length.out = cut)]
yt <- reduced
#complete holts winter smoothing
for (i in 1:m) {
lv[i+1] <- aa*yt[i] + (1-aa)*(lv[i]+bv[i])
bv[i+1] <- bb*(lv[i+1]-lv[i]) + (1-bb)*bv[i]
}
for (i in (m+1):ti) {
lv[i+1] <- aa*(yt[i]-sv[i+1-m]) + (1-aa)*(lv[i]+bv[i])
bv[i+1] <- bb*(lv[i+1]-lv[i]) + (1-bb)*bv[i]
sv[i+1] <- gg*(yt[i]-lv[i]-bv[i]) + (1-gg)*sv[i+1-m]
}
#forecast for a duration defined above
kk <- seq(1,fc,1)
ik <- floor((kk-1)/m)
yf <- lv[i+1] + bv[i+1] * kk + sv[i+1+kk-m*(ik+1)]
#combine and interpolate data to return it to the previous resolution
modelled_data <- c(lv,yf)
return_data <- interpolate_data(modelled_data, round(length(data)*((cut+fc)/cut)))
return(return_data)
}
#function to fill in space with future dates
get_dates_sequence <- function(start_date, length_of_list) {
date_sequence <- seq(as.Date(start_date), by = "days", length.out = length_of_list)
return(date_sequence)
}
#define the dates for which we are fitting the holt-winters forecast
dates <- data$Date
start_date_index <- which(data$Date == '2020-07-01')
end_date_index <- length(data$Date)
#prepare all the data for the forecast plot
#WC1
wc1.obs <- smooth_data(data$WC1,0.025,1250)[start_date_index:end_date_index]
wc1.model <- holtwinter(wc1.obs)
pad <- rep(NaN,length(wc1.model)-length(wc1))
wc1.obs <- c(wc1.obs,pad)
#wc2
wc2.obs <- smooth_data(data$WC2,0.025,1250)[start_date_index:end_date_index]
wc2.model <- holtwinter(wc2.obs)
wc2.obs <- c(wc2.obs,pad)
#ec1
ec1.obs <- smooth_data(data$EC1,0.025,1250)[start_date_index:end_date_index]
ec1.model <- holtwinter(ec1.obs)
ec1.obs <- c(ec1.obs,pad)
#ec2
ec2.obs <- smooth_data(data$EC2,0.025,1250)[start_date_index:end_date_index]
ec2.model <- holtwinter(ec2.obs)
ec2.obs <- c(ec2.obs,pad)
#ec3
ec3.obs <- smooth_data(data$EC3,0.025,1250)[start_date_index:end_date_index]
ec3.model <- holtwinter(ec3.obs)
ec3.obs <- c(ec3.obs,pad)
#ec4
ec4.obs <- smooth_data(data$EC4,0.025,1250)[start_date_index:end_date_index]
ec4.model <- holtwinter(ec4.obs)
ec4.obs <- c(ec4.obs,pad)
#Date
date.fc <- data$Date[start_date_index:end_date_index]
date_pad <- as.character((get_dates_sequence(date.fc[length(date.fc)], ((length(wc1.model)-length(date.fc))+1)))[-1])
date.fc <- append(date.fc,date_pad)
date.fc
date[1]
date.fc[1]
date.fc[(length(date.fc))]
date.fc[2*(length(date.fc))/3]
date.fc[(length(date.fc))/3]
library(shiny); runApp('app-master.R')
